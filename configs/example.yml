Global:
  use_gpu: true
  epochs: 3
  checkpoint: &v_checkpoint  './checkpoint/esmm'
  results_dir: './results'
  # change the base directory of your data.
  data_base: &base /data/yezheng/ctr/x-deeplearning/xdl-algorithm-solution/ESMM/data/build/
#  data_base: &base /mnt/d/datasets/ctr/tfrecord/

Preprocess:
  Topic:
    base_dir: &base_dir '/base/dir/'
    topic_name: !join [*base_dir, 'topic.txt']
    output_name: !join [*base_dir, 'topic_stemmed.txt']
    is_stemming: true

Models:
  active: 'esmm' # which model is active. put multiple model in one place
  'esmm':
    embed_size: 18
    feature_dim: 1000
    min_by: 3  # use to decide the hash bucket, if the feature_dim is far less than 1000, we can reduce the waste.
    share_embedding: true
    class_weight: [1., 1.]
    mlp_ctr: [200, 80]  # [360, 200, 80]: origin paper config.
    mlp_cvr: [200, 80]
    dropouts: [] # [0.5, 0.5, 0.5]
    batch_norm: false
    activation:
      name: LeakyReLU
      paras:
        alpha: 0.25 # 0.25 is the default value in x-deeplearning

Loss:
  key: ['ctr_output', 'ctcvr_pred', 'cvr_output']
  # [direct_auc_loss, direct_auc_loss, fake_loss], [sparse_categorical_crossentropy, sparse_categorical_crossentropy, sparse_categorical_crossentropy]
  value: [binary_crossentropy, binary_crossentropy, binary_crossentropy_cvr]
  weights: [1., 1., 0.]


Optimizer:
  active: Adam
  Adam:
    lr: &lr 0.001
    epsilon: 1.e-08
    decay: 0.0001
  Adamax:
    lr: *lr
  Nadam:
    lr: *lr

Metric:
  name: ClsMetric
  main_indicator: acc

Callbacks: # has to be in order, not used currently.
  - ModelCheckpoint:
      filepath: !join [*v_checkpoint, '/hello']
      save_best_only: false
      save_freq: 'epoch'
      monitor: 'val_loss'
      verbose: 1
      mode: 'min'
  - ReduceLROnPlateau:
      monitor: 'val_loss'
  - EarlyStopping:
      patience: 1

Train:
  dataset:
    names: [!join [*base, 'train.0'],
            !join [*base, 'train.1'],
            !join [*base, 'train.2'],
            !join [*base, 'train.3'],
            !join [*base, 'train.4']
    ]
    batch_size: &batch_size 5000
    shuffle: true
    buffer_size: 1000 * 5 * 10

Eval:
  dataset:
    names: [!join [*base, 'test.0'],
            !join [*base, 'test.1'],
            !join [*base, 'test.2'],
            !join [*base, 'test.3'],
            !join [*base, 'test.4']
    ]
    batch_size: *batch_size